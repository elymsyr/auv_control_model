{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvE3H0wkj23S"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tgpdUm9rlvRU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import h5py\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX9-DPRambsP"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# %cd /content/drive/My Drive/underwater/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "config = {\n",
        "    'batch_size': 256,\n",
        "    'lr': 3e-4,\n",
        "    'epochs': 50,\n",
        "    'weight_decay': 1e-6,\n",
        "    'patience': 15\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZEQXl9ejvoy"
      },
      "outputs": [],
      "source": [
        "\n",
        "def visualize_results(targets_denorm, predictions_denorm):\n",
        "    # Create subplots for scatter plots\n",
        "    plt.figure(figsize=(18, 20))\n",
        "    \n",
        "    print(\"Targets shape:\", targets_denorm.shape)\n",
        "    print(\"Predictions shape:\", predictions_denorm.shape)\n",
        "\n",
        "    # Scatter plots of True vs Predicted values\n",
        "    for i in range(8):\n",
        "        plt.subplot(4, 2, i+1)\n",
        "        true = targets_denorm[:, i]\n",
        "        pred = predictions_denorm[:, i]\n",
        "        plt.scatter(true, pred, alpha=0.3, label='Samples')\n",
        "        plt.plot([min(true), max(true)], [min(true), max(true)], 'r--', label='Perfect Prediction')\n",
        "        plt.xlabel(f'True Value (Thruster {i+1})')\n",
        "        plt.ylabel(f'Predicted Value (Thruster {i+1})')\n",
        "        plt.title(f'Thruster {i+1} - True vs Predicted')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Add R² and MAE to plot\n",
        "        r2 = r2_score(true, pred)\n",
        "        mae = mean_absolute_error(true, pred)\n",
        "        plt.text(0.05, 0.9, f'R²: {r2:.2f}\\nMAE: {mae:.2f}',\n",
        "                transform=plt.gca().transAxes,\n",
        "                bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('true_vs_predicted_scatter.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Create sample comparison plot (first 100 samples)\n",
        "    plt.figure(figsize=(18, 20))\n",
        "    sample_indices = np.arange(100)\n",
        "\n",
        "    for i in range(8):\n",
        "        plt.subplot(4, 2, i+1)\n",
        "        plt.plot(sample_indices, targets_denorm[:100, i], 'b-', label='True')\n",
        "        plt.plot(sample_indices, predictions_denorm[:100, i], 'r--', label='Predicted')\n",
        "        plt.xlabel('Sample Index')\n",
        "        plt.ylabel('Value')\n",
        "        plt.title(f'Thruster {i+1} - First 100 Samples')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sample_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Residuals distribution\n",
        "    plt.figure(figsize=(18, 20))\n",
        "    residuals = targets_denorm - predictions_denorm\n",
        "\n",
        "    for i in range(8):\n",
        "        plt.subplot(4, 2, i+1)\n",
        "        plt.hist(residuals[:, i], bins=50, alpha=0.7)\n",
        "        plt.xlabel('Residual (True - Predicted)')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title(f'Thruster {i+1} - Residual Distribution')\n",
        "        plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('residual_distribution.png')\n",
        "    plt.show()\n",
        "\n",
        "# Configuration for feature normalization\n",
        "class FeatureNormalizer:\n",
        "    def __init__(self, x_scales, y_scales):\n",
        "        self.x_scales = x_scales\n",
        "        self.y_scales = y_scales\n",
        "\n",
        "    def normalize_x(self, x):\n",
        "        return x / self.x_scales\n",
        "\n",
        "    def denormalize_x(self, x_norm):\n",
        "        return x_norm * self.x_scales\n",
        "\n",
        "    def normalize_y(self, y):\n",
        "        return y / self.y_scales\n",
        "\n",
        "    def denormalize_y(self, y_norm):\n",
        "        return y_norm * self.y_scales\n",
        "\n",
        "NU_MIN: float  = 0.0\n",
        "NU_MAX: float  = 5.0\n",
        "D_LOC_MAX: float  = 5.0\n",
        "DEPTH_MIN: float  = 2.0\n",
        "DEPTH_MAX: float  = 25.0\n",
        "ZERO: float = 1.0\n",
        "\n",
        "# Example scaling configuration\n",
        "X_SCALE_FACTORS = [\n",
        "    np.pi/4, np.pi/4, np.pi,\n",
        "    NU_MAX, NU_MAX, NU_MAX,\n",
        "    0.05, 0.05, 0.1,\n",
        "]\n",
        "\n",
        "X_REF_SCALE_FACTORS = [\n",
        "    D_LOC_MAX, D_LOC_MAX, DEPTH_MAX,\n",
        "    ZERO, ZERO, np.pi,\n",
        "    NU_MAX, NU_MAX, ZERO,\n",
        "    ZERO, ZERO, ZERO\n",
        "]\n",
        "\n",
        "x_ref_scale = np.array(X_REF_SCALE_FACTORS*41, dtype=np.float32)\n",
        "\n",
        "x_scales = np.hstack((X_SCALE_FACTORS, x_ref_scale))\n",
        "\n",
        "Y_SCALE_FACTORS = [80.0, 80.0, 80.0, 80.0,  # Main thrusters\n",
        "                   50.0, 50.0, 50.0, 50.0]   # Tunnel thrusters\n",
        "normalizer = FeatureNormalizer(x_scales, np.array(Y_SCALE_FACTORS, dtype=np.float32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8IKP7W-mFcq"
      },
      "outputs": [],
      "source": [
        "with h5py.File('data/test_data.h5', 'r') as hf:\n",
        "    # Load datasets\n",
        "    x_current = hf['x_current'][:]  # Shape: (num_samples, 12)\n",
        "    x_ref = hf['x_ref'][:]          # Shape: (num_samples, 12*(N+1))\n",
        "    u_opt = hf['u_opt'][:]          # Shape: (num_samples, 8)\n",
        "    \n",
        "    # Reshape x_ref into 3D array: (samples, horizon+1, state_dim)\n",
        "    x_path = x_ref.reshape(x_ref.shape[0], 12, 41).transpose(0, 2, 1)\n",
        "\n",
        "    for i, row in enumerate(x_current):\n",
        "        pos_array = np.array([row[0], row[1], row[2], 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=np.float32)\n",
        "        x_path[i] -= pos_array\n",
        "    \n",
        "    x_current = x_current[:, 3:]\n",
        "\n",
        "    X = np.hstack((x_current, x_ref)).astype(np.float32)\n",
        "    y = u_opt.astype(np.float32)\n",
        "\n",
        "# Split dataset into train (60%), val (20%), test (20%)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, shuffle=True, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, shuffle=True, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "\n",
        "# Apply normalization\n",
        "X_train_norm = normalizer.normalize_x(X_train)\n",
        "X_val_norm = normalizer.normalize_x(X_val)\n",
        "X_test_norm = normalizer.normalize_x(X_test)\n",
        "y_train_norm = normalizer.normalize_y(y_train)\n",
        "y_val_norm = normalizer.normalize_y(y_val)\n",
        "y_test_norm = normalizer.normalize_y(y_test)\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_dataset = TensorDataset(torch.FloatTensor(X_train_norm), torch.FloatTensor(y_train_norm))\n",
        "val_dataset = TensorDataset(torch.FloatTensor(X_val_norm), torch.FloatTensor(y_val_norm))\n",
        "test_dataset = TensorDataset(torch.FloatTensor(X_test_norm), torch.FloatTensor(y_test_norm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV31Mvs2mPjz"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayDmuZllmU5M"
      },
      "outputs": [],
      "source": [
        "class FossenNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FossenNet, self).__init__()\n",
        "        # State processing branch (current state: 9 features)\n",
        "        self.state_net = nn.Sequential(\n",
        "            nn.Linear(9, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "        \n",
        "        # Reference trajectory processing (492 features -> 41 timesteps x 12 features)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=12,\n",
        "            hidden_size=128,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=0.2\n",
        "        )\n",
        "        self.ref_fc = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "        \n",
        "        # Combined processing\n",
        "        self.combined_net = nn.Sequential(\n",
        "            nn.Linear(32 + 64, 128),  # Concatenated features\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(64, 8)\n",
        "        )\n",
        "        \n",
        "        # Initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0.1)\n",
        "            elif isinstance(m, nn.LSTM):\n",
        "                for name, param in m.named_parameters():\n",
        "                    if 'weight_ih' in name:\n",
        "                        nn.init.xavier_uniform_(param.data)\n",
        "                    elif 'weight_hh' in name:\n",
        "                        nn.init.orthogonal_(param.data)\n",
        "                    elif 'bias' in name:\n",
        "                        param.data.fill_(0)\n",
        "                        # Initialize forget gate bias to 1\n",
        "                        n = param.size(0)\n",
        "                        param.data[n//4:n//2].fill_(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Split input: [state (9), ref_trajectory (492)]\n",
        "        state = x[:, :9]\n",
        "        ref = x[:, 9:9+492]\n",
        "        \n",
        "        # Process current state\n",
        "        state_out = self.state_net(state)\n",
        "        \n",
        "        # Process reference trajectory\n",
        "        batch_size = x.size(0)\n",
        "        ref = ref.view(batch_size, 41, 12)  # (batch, seq_len, features)\n",
        "        lstm_out, _ = self.lstm(ref)        # (batch, seq_len, hidden_size)\n",
        "        ref_out = lstm_out[:, -1, :]         # Last timestep output\n",
        "        ref_out = self.ref_fc(ref_out)\n",
        "        \n",
        "        # Combine features and predict\n",
        "        combined = torch.cat((state_out, ref_out), dim=1)\n",
        "        return self.combined_net(combined)\n",
        "\n",
        "def train():\n",
        "    model = FossenNet()\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=7, factor=0.5)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], num_workers=2)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    no_improve = 0\n",
        "\n",
        "    for epoch in range(config['epochs']):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(x_batch)\n",
        "            loss = criterion(pred, y_batch)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for x_val, y_val in val_loader:\n",
        "                pred = model(x_val)\n",
        "                val_loss += criterion(pred, y_val).item()\n",
        "\n",
        "        avg_train = train_loss/len(train_loader)\n",
        "        avg_val = val_loss/len(val_loader)\n",
        "        scheduler.step(avg_val)\n",
        "\n",
        "        # Early stopping check\n",
        "        if avg_val < best_loss:\n",
        "            best_loss = avg_val\n",
        "            no_improve = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "        else:\n",
        "            no_improve += 1\n",
        "\n",
        "        if no_improve >= config['patience']:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['epochs']} | \"\n",
        "              f\"Train: {avg_train:.4f} | Val: {avg_val:.4f} | \"\n",
        "              f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "\n",
        "    model.to('cuda')\n",
        "    scripted_model = torch.jit.script(model)\n",
        "    scripted_model.save('fossen_net_scripted.pt')\n",
        "\n",
        "def evaluate_test_set():\n",
        "    model = FossenNet()\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2)\n",
        "\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in test_loader:\n",
        "            pred = model(x_batch)\n",
        "            predictions.append(pred.numpy())\n",
        "            targets.append(y_batch.numpy())\n",
        "\n",
        "    predictions = np.vstack(predictions)\n",
        "    targets = np.vstack(targets)\n",
        "\n",
        "    # Denormalize\n",
        "    predictions_denorm = normalizer.denormalize_y(predictions)\n",
        "    targets_denorm = normalizer.denormalize_y(targets)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'MAE': mean_absolute_error(targets_denorm, predictions_denorm),\n",
        "        'MSE': mean_squared_error(targets_denorm, predictions_denorm),\n",
        "        'RMSE': np.sqrt(mean_squared_error(targets_denorm, predictions_denorm)),\n",
        "        'R2': r2_score(targets_denorm, predictions_denorm)\n",
        "    }\n",
        "\n",
        "    # Per-thruster metrics\n",
        "    thruster_metrics = []\n",
        "    for i in range(8):\n",
        "        thruster_metrics.append({\n",
        "            'Thruster': i+1,\n",
        "            'MAE': mean_absolute_error(targets_denorm[:, i], predictions_denorm[:, i]),\n",
        "            'MSE': mean_squared_error(targets_denorm[:, i], predictions_denorm[:, i]),\n",
        "            'RMSE': np.sqrt(mean_squared_error(targets_denorm[:, i], predictions_denorm[:, i])),\n",
        "            'R2': r2_score(targets_denorm[:, i], predictions_denorm[:, i])\n",
        "        })\n",
        "\n",
        "    return metrics, thruster_metrics, predictions_denorm, targets_denorm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEYj5o5FxURO"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgs7lnmemWtu"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    train()\n",
        "\n",
        "    # Evaluate on test set\n",
        "    metrics, thruster_metrics, preds, targets = evaluate_test_set()\n",
        "\n",
        "    print(\"\\nFinal Test Set Metrics:\")\n",
        "    print(f\"MAE: {metrics['MAE']:.4f}\")\n",
        "    print(f\"MSE: {metrics['MSE']:.4f}\")\n",
        "    print(f\"RMSE: {metrics['RMSE']:.4f}\")\n",
        "    print(f\"R²: {metrics['R2']:.4f}\")\n",
        "\n",
        "    print(\"\\nPer-Thruster Metrics:\")\n",
        "    for tm in thruster_metrics:\n",
        "        print(f\"\\nThruster {tm['Thruster']}:\")\n",
        "        print(f\"MAE: {tm['MAE']:.4f}  MSE: {tm['MSE']:.4f}\")\n",
        "        print(f\"RMSE: {tm['RMSE']:.4f}  R²: {tm['R2']:.4f}\")\n",
        "\n",
        "    visualize_results(targets, preds)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mppi",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
