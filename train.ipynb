{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvE3H0wkj23S"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tgpdUm9rlvRU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import h5py\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.8.0+cu128\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nX9-DPRambsP"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# %cd /content/drive/My Drive/underwater/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model folder 'pretrained/fossen_net_1' recreated (no trained model found).\n"
          ]
        }
      ],
      "source": [
        "# # Training configuration\n",
        "# config = {\n",
        "#     'batch_size': 256,\n",
        "#     'lr': 3e-4,\n",
        "#     'epochs': 50,\n",
        "#     'weight_decay': 1e-6,\n",
        "#     'patience': 15\n",
        "# }\n",
        "\n",
        "model_name = \"pretrained/fossen_net_1\"\n",
        "data_path = \"data/data.h5\"\n",
        "SAMPLES_PER_SCENARIO = 100\n",
        "\n",
        "if os.path.exists(model_name):\n",
        "    pt_files = glob.glob(os.path.join(model_name, \"*.pt\"))\n",
        "    if pt_files:\n",
        "        sys.exit(f\"ERROR: Trained model found in '{model_name}'! Aborting to avoid overwrite.\")\n",
        "    else:\n",
        "        shutil.rmtree(model_name)\n",
        "        os.makedirs(model_name)\n",
        "        print(f\"Model folder '{model_name}' recreated (no trained model found).\")\n",
        "else:\n",
        "    # Create folder if it doesn't exist\n",
        "    os.makedirs(model_name)\n",
        "    print(f\"Model folder '{model_name}' created successfully.\")\n",
        "\n",
        "config = {\n",
        "    'batch_size': 256,          # 128 -> 256\n",
        "    'lr': 3e-4,                 # Lower learning rate\n",
        "    'epochs': 50,               # Increased epochs to allow for slower convergence\n",
        "    'weight_decay': 1e-6,\n",
        "    'patience': 20,\n",
        "    'optimizer': 'Adam',        # Switch to Adam\n",
        "    'loss_function': 'Huber'    # Use HuberLoss\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HZEQXl9ejvoy"
      },
      "outputs": [],
      "source": [
        "\n",
        "def visualize_results(targets_denorm, predictions_denorm):\n",
        "    # Create subplots for scatter plots\n",
        "    plt.figure(figsize=(18, 20))\n",
        "    \n",
        "    print(\"Targets shape:\", targets_denorm.shape)\n",
        "    print(\"Predictions shape:\", predictions_denorm.shape)\n",
        "\n",
        "    # Scatter plots of True vs Predicted values\n",
        "    for i in range(8):\n",
        "        plt.subplot(4, 2, i+1)\n",
        "        true = targets_denorm[:, i]\n",
        "        pred = predictions_denorm[:, i]\n",
        "        plt.scatter(true, pred, alpha=0.3, label='Samples')\n",
        "        plt.plot([min(true), max(true)], [min(true), max(true)], 'r--', label='Perfect Prediction')\n",
        "        plt.xlabel(f'True Value (Thruster {i+1})')\n",
        "        plt.ylabel(f'Predicted Value (Thruster {i+1})')\n",
        "        plt.title(f'Thruster {i+1} - True vs Predicted')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Add R² and MAE to plot\n",
        "        r2 = r2_score(true, pred)\n",
        "        mae = mean_absolute_error(true, pred)\n",
        "        plt.text(0.05, 0.9, f'R²: {r2:.2f}\\nMAE: {mae:.2f}',\n",
        "                transform=plt.gca().transAxes,\n",
        "                bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{model_name}/true_vs_predicted_scatter.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Create sample comparison plot (first 100 samples)\n",
        "    plt.figure(figsize=(18, 20))\n",
        "    sample_indices = np.arange(100)\n",
        "\n",
        "    for i in range(8):\n",
        "        plt.subplot(4, 2, i+1)\n",
        "        plt.plot(sample_indices, targets_denorm[:100, i], 'b-', label='True')\n",
        "        plt.plot(sample_indices, predictions_denorm[:100, i], 'r--', label='Predicted')\n",
        "        plt.xlabel('Sample Index')\n",
        "        plt.ylabel('Value')\n",
        "        plt.title(f'Thruster {i+1} - First 100 Samples')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{model_name}/sample_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Residuals distribution\n",
        "    plt.figure(figsize=(18, 20))\n",
        "    residuals = targets_denorm - predictions_denorm\n",
        "\n",
        "    for i in range(8):\n",
        "        plt.subplot(4, 2, i+1)\n",
        "        plt.hist(residuals[:, i], bins=50, alpha=0.7)\n",
        "        plt.xlabel('Residual (True - Predicted)')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title(f'Thruster {i+1} - Residual Distribution')\n",
        "        plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{model_name}/residual_distribution.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h8IKP7W-mFcq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total scenarios: 3766\n",
            "Train scenarios: 2259, Val scenarios: 753, Test scenarios: 754\n",
            "Fitted scalers have been saved.\n",
            "X batch shape: torch.Size([64, 501])\n",
            "y batch shape: torch.Size([64, 8])\n"
          ]
        }
      ],
      "source": [
        "with h5py.File(data_path, 'r') as hf:\n",
        "    num_samples = hf['x_current'].shape[0]\n",
        "    num_scenarios = num_samples // SAMPLES_PER_SCENARIO\n",
        "    scenario_ids = np.arange(num_scenarios)\n",
        "\n",
        "# Train / Val / Test split\n",
        "train_val_ids, test_ids = train_test_split(\n",
        "    scenario_ids, test_size=0.2, shuffle=True, random_state=42\n",
        ")\n",
        "train_ids, val_ids = train_test_split(\n",
        "    train_val_ids, test_size=0.25, shuffle=True, random_state=42\n",
        ")  # 0.25 * 0.8 = 0.2\n",
        "\n",
        "def scenario_ids_to_indices(scenario_ids):\n",
        "    return np.concatenate([\n",
        "        np.arange(i * SAMPLES_PER_SCENARIO, (i + 1) * SAMPLES_PER_SCENARIO)\n",
        "        for i in scenario_ids\n",
        "    ])\n",
        "\n",
        "train_indices = scenario_ids_to_indices(train_ids)\n",
        "val_indices = scenario_ids_to_indices(val_ids)\n",
        "test_indices = scenario_ids_to_indices(test_ids)\n",
        "\n",
        "print(f\"Total scenarios: {num_scenarios}\")\n",
        "print(f\"Train scenarios: {len(train_ids)}, Val scenarios: {len(val_ids)}, Test scenarios: {len(test_ids)}\")\n",
        "\n",
        "# -----------------------\n",
        "# Step 2: Fit scalers using only train set\n",
        "# -----------------------\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "with h5py.File(data_path, 'r') as hf:\n",
        "    # Read only train data in chunks to fit scalers\n",
        "    for idx in train_indices:\n",
        "        x_current = hf['x_current'][idx]\n",
        "        x_ref = hf['x_ref'][idx]\n",
        "        u_opt = hf['u_opt'][idx]\n",
        "\n",
        "        # Preprocessing\n",
        "        x_path_3d = x_ref.reshape(12, 41).T  # (41, 12)\n",
        "        pos_array = np.zeros(12, dtype=np.float32)\n",
        "        pos_array[:3] = x_current[:3]\n",
        "        x_path_3d -= pos_array\n",
        "        x_ref_relative = x_path_3d.T.reshape(-1)\n",
        "\n",
        "        X_sample = np.hstack((x_current[3:], x_ref_relative)).astype(np.float32)\n",
        "        y_sample = u_opt.astype(np.float32)\n",
        "\n",
        "        scaler_X.partial_fit(X_sample.reshape(1, -1))\n",
        "        scaler_y.partial_fit(y_sample.reshape(1, -1))\n",
        "\n",
        "# Save scalers\n",
        "joblib.dump(scaler_X, f'{model_name}/scaler_X.gz')\n",
        "joblib.dump(scaler_y, f'{model_name}/scaler_y.gz')\n",
        "print(\"Fitted scalers have been saved.\")\n",
        "\n",
        "# -----------------------\n",
        "# Step 3: Dataset class for lazy loading\n",
        "# -----------------------\n",
        "class H5Dataset(Dataset):\n",
        "    def __init__(self, file_path, indices, scaler_X, scaler_y):\n",
        "        self.file_path = file_path\n",
        "        self.indices = indices\n",
        "        self.scaler_X = scaler_X\n",
        "        self.scaler_y = scaler_y\n",
        "        self.hf = None  # will open in __getitem__\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.hf is None:\n",
        "            self.hf = h5py.File(self.file_path, 'r')\n",
        "\n",
        "        data_idx = self.indices[idx]\n",
        "        x_current = self.hf['x_current'][data_idx]\n",
        "        x_ref = self.hf['x_ref'][data_idx]\n",
        "        u_opt = self.hf['u_opt'][data_idx]\n",
        "\n",
        "        # Preprocessing (same as original)\n",
        "        x_path_3d = x_ref.reshape(12, 41).T\n",
        "        pos_array = np.zeros(12, dtype=np.float32)\n",
        "        pos_array[:3] = x_current[:3]\n",
        "        x_path_3d -= pos_array\n",
        "        x_ref_relative = x_path_3d.T.reshape(-1)\n",
        "\n",
        "        X = np.hstack((x_current[3:], x_ref_relative)).astype(np.float32)\n",
        "        y = u_opt.astype(np.float32)\n",
        "\n",
        "        # Normalize\n",
        "        X_norm = self.scaler_X.transform(X.reshape(1, -1)).squeeze(0)\n",
        "        y_norm = self.scaler_y.transform(y.reshape(1, -1)).squeeze(0)\n",
        "\n",
        "        return torch.from_numpy(X_norm), torch.from_numpy(y_norm)\n",
        "\n",
        "    def __del__(self):\n",
        "        if self.hf is not None:\n",
        "            self.hf.close()\n",
        "\n",
        "# -----------------------\n",
        "# Step 4: Create DataLoaders\n",
        "# -----------------------\n",
        "train_dataset = H5Dataset(data_path, train_indices, scaler_X, scaler_y)\n",
        "val_dataset = H5Dataset(data_path, val_indices, scaler_X, scaler_y)\n",
        "test_dataset = H5Dataset(data_path, test_indices, scaler_X, scaler_y)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# -----------------------\n",
        "# Example usage\n",
        "# -----------------------\n",
        "for X_batch, y_batch in train_loader:\n",
        "    print(\"X batch shape:\", X_batch.shape)\n",
        "    print(\"y batch shape:\", y_batch.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV31Mvs2mPjz"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ayDmuZllmU5M"
      },
      "outputs": [],
      "source": [
        "# Dropout layers are disabled temporarily to see if the training loss drops below the validation loss. If it does, your dropout rates are likely too high.\n",
        "\n",
        "class FossenNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FossenNet, self).__init__()\n",
        "        # State processing branch (current state: 9 features)\n",
        "        self.state_net = nn.Sequential(\n",
        "            nn.Linear(9, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            # nn.Dropout(0.2), # Temporarily disabled\n",
        "            nn.Linear(64, 32),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "        \n",
        "        # Reference trajectory processing (492 features -> 41 timesteps x 12 features)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=12,\n",
        "            hidden_size=128,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=0.2  # Note: This dropout is within the LSTM layers and is often beneficial\n",
        "        )\n",
        "        self.ref_fc = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "        \n",
        "        # --- MODIFIED: Increased Capacity ---\n",
        "        # Combined processing\n",
        "        self.combined_net = nn.Sequential(\n",
        "            nn.Linear(32 + 64, 256),  # Concatenated features -> Increased width\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            # nn.Dropout(0.3), # Temporarily disabled\n",
        "            nn.Linear(256, 128),      # Added another layer for more depth\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(64, 8)\n",
        "        )\n",
        "        \n",
        "        # Initialize weights (no changes here)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0.1)\n",
        "            elif isinstance(m, nn.LSTM):\n",
        "                for name, param in m.named_parameters():\n",
        "                    if 'weight_ih' in name:\n",
        "                        nn.init.xavier_uniform_(param.data)\n",
        "                    elif 'weight_hh' in name:\n",
        "                        nn.init.orthogonal_(param.data)\n",
        "                    elif 'bias' in name:\n",
        "                        param.data.fill_(0)\n",
        "                        n = param.size(0)\n",
        "                        param.data[n//4:n//2].fill_(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        state = x[:, :9]\n",
        "        ref = x[:, 9:9+492]\n",
        "        \n",
        "        state_out = self.state_net(state)\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "        ref = ref.view(batch_size, 41, 12)\n",
        "        lstm_out, _ = self.lstm(ref)\n",
        "        ref_out = lstm_out[:, -1, :]\n",
        "        ref_out = self.ref_fc(ref_out)\n",
        "        \n",
        "        combined = torch.cat((state_out, ref_out), dim=1)\n",
        "        return self.combined_net(combined)\n",
        "\n",
        "def train(config, train_dataset, val_dataset):\n",
        "    model = FossenNet()\n",
        "    \n",
        "    # --- MODIFIED: Select Loss Function based on config ---\n",
        "    if config['loss_function'] == 'Huber':\n",
        "        criterion = nn.HuberLoss()\n",
        "    else: # Default to MSE\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "    # --- MODIFIED: Select Optimizer based on config ---\n",
        "    if config['optimizer'] == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "    else: # Default to AdamW\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "        \n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=7, factor=0.5)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], num_workers=2)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    no_improve = 0\n",
        "    \n",
        "    print(\"--- Starting Training ---\")\n",
        "    print(f\"Configuration: {config}\")\n",
        "\n",
        "    for epoch in range(config['epochs']):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(x_batch)\n",
        "            loss = criterion(pred, y_batch)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for x_val, y_val in val_loader:\n",
        "                pred = model(x_val)\n",
        "                val_loss += criterion(pred, y_val).item()\n",
        "\n",
        "        avg_train = train_loss / len(train_loader)\n",
        "        avg_val = val_loss / len(val_loader)\n",
        "        scheduler.step(avg_val)\n",
        "\n",
        "        # Early stopping check\n",
        "        if avg_val < best_loss:\n",
        "            best_loss = avg_val\n",
        "            no_improve = 0\n",
        "            torch.save(model.state_dict(), f'{model_name}/best_model_huber_adam.pth')\n",
        "        else:\n",
        "            no_improve += 1\n",
        "\n",
        "        if no_improve >= config['patience']:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['epochs']} | \"\n",
        "              f\"Train Loss: {avg_train:.6f} | Val Loss: {avg_val:.6f} | \"\n",
        "              f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "\n",
        "    model.to('cuda')\n",
        "    scripted_model = torch.jit.script(model)\n",
        "    scripted_model.save(f'{model_name}/fossen_net_scripted.pt')\n",
        "\n",
        "def evaluate_test_set():\n",
        "    # Load model\n",
        "    model = FossenNet()\n",
        "    model.load_state_dict(torch.load(f'{model_name}/best_model_huber_adam.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    # Load saved scalers\n",
        "    scaler_X = joblib.load(f'{model_name}/scaler_X.gz')\n",
        "    scaler_y = joblib.load(f'{model_name}/scaler_y.gz')\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2)\n",
        "\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in test_loader:\n",
        "            pred = model(x_batch)\n",
        "            predictions.append(pred.numpy())\n",
        "            targets.append(y_batch.numpy())\n",
        "\n",
        "    predictions = np.vstack(predictions)\n",
        "    targets = np.vstack(targets)\n",
        "\n",
        "    # --- Denormalize (rescale back to original) ---\n",
        "    predictions_denorm = scaler_y.inverse_transform(predictions)\n",
        "    targets_denorm = scaler_y.inverse_transform(targets)\n",
        "\n",
        "    # --- Metrics ---\n",
        "    metrics = {\n",
        "        'MAE': mean_absolute_error(targets_denorm, predictions_denorm),\n",
        "        'MSE': mean_squared_error(targets_denorm, predictions_denorm),\n",
        "        'RMSE': np.sqrt(mean_squared_error(targets_denorm, predictions_denorm)),\n",
        "        'R2': r2_score(targets_denorm, predictions_denorm)\n",
        "    }\n",
        "\n",
        "    thruster_metrics = []\n",
        "    for i in range(8):\n",
        "        thruster_metrics.append({\n",
        "            'Thruster': i+1,\n",
        "            'MAE': mean_absolute_error(targets_denorm[:, i], predictions_denorm[:, i]),\n",
        "            'MSE': mean_squared_error(targets_denorm[:, i], predictions_denorm[:, i]),\n",
        "            'RMSE': np.sqrt(mean_squared_error(targets_denorm[:, i], predictions_denorm[:, i])),\n",
        "            'R2': r2_score(targets_denorm[:, i], predictions_denorm[:, i])\n",
        "        })\n",
        "\n",
        "    return metrics, thruster_metrics, predictions_denorm, targets_denorm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEYj5o5FxURO"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jgs7lnmemWtu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Training ---\n",
            "Configuration: {'batch_size': 256, 'lr': 0.0003, 'epochs': 50, 'weight_decay': 1e-06, 'patience': 20, 'optimizer': 'Adam', 'loss_function': 'Huber'}\n",
            "Epoch 1/50 | Train Loss: 0.081318 | Val Loss: 0.052989 | LR: 3.00e-04\n",
            "Epoch 2/50 | Train Loss: 0.044742 | Val Loss: 0.035116 | LR: 3.00e-04\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[32m      5\u001b[39m     metrics, thruster_metrics, preds, targets = evaluate_test_set()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(config, train_dataset, val_dataset)\u001b[39m\n\u001b[32m    107\u001b[39m pred = model(x_batch)\n\u001b[32m    108\u001b[39m loss = criterion(pred, y_batch)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m    111\u001b[39m optimizer.step()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/control_model/lib/python3.11/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/control_model/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/control_model/lib/python3.11/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    train(config, train_dataset, val_dataset)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    metrics, thruster_metrics, preds, targets = evaluate_test_set()\n",
        "\n",
        "    print(\"\\nFinal Test Set Metrics:\")\n",
        "    print(f\"MAE: {metrics['MAE']:.4f}\")\n",
        "    print(f\"MSE: {metrics['MSE']:.4f}\")\n",
        "    print(f\"RMSE: {metrics['RMSE']:.4f}\")\n",
        "    print(f\"R²: {metrics['R2']:.4f}\")\n",
        "\n",
        "    print(\"\\nPer-Thruster Metrics:\")\n",
        "    for tm in thruster_metrics:\n",
        "        print(f\"\\nThruster {tm['Thruster']}:\")\n",
        "        print(f\"MAE: {tm['MAE']:.4f}  MSE: {tm['MSE']:.4f}\")\n",
        "        print(f\"RMSE: {tm['RMSE']:.4f}  R²: {tm['R2']:.4f}\")\n",
        "\n",
        "    visualize_results(targets, preds)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "control_model",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
